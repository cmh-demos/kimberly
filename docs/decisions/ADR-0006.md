# ADR-0006: Hybrid LLM Integration Approach

## Status
Accepted

## Context
Kimberly requires AI capabilities for conversational chat and agent delegation while maintaining free-mode operation and cost control. The system needs to balance performance, cost, privacy, and operational complexity.

## Decision
Implement a hybrid LLM integration approach:

**Primary Strategy:** Local inference using Llama 3.1 8B quantized model
- Zero API costs, full privacy control, works offline
- Aligns with free-mode goals and user data sovereignty
- Complete customization and control over AI behavior

**Fallback Strategy:** API wrapper (OpenAI-compatible) for high-demand scenarios
- Handles peak loads or complex queries beyond local model capabilities
- Provides reliability when local inference is insufficient
- Enables graceful degradation rather than complete failure

**Latency Targets:**
- Chat Response: <1 second (warm model)
- Initial Model Load: <30 seconds (acceptable for first use)
- Memory Retrieval: <500ms
- Agent Delegation: <5 seconds (complex tasks)
- Meditation Process: <10 minutes (background, non-blocking)

## Consequences
- **Pros:** Cost control, privacy protection, offline capability, predictable performance
- **Cons:** Higher resource requirements, operational complexity, potential cold-start delays
- **Risks:** Local model limitations may require API fallback tuning, hardware requirements may limit deployment options
- **Benefits:** Aligns with project goals of free hosting, maintains user trust through local processing

## Implementation Notes
- Model selection will be finalized after licensing review
- Infrastructure must support GPU acceleration for optimal performance
- Monitoring required for local vs API usage patterns
- User consent required for any API fallback usage